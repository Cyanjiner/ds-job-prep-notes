# My DS Summer Intern Preparation Guide

```{r setup2, include=FALSE}
library(knitr)
library(reticulate)
knitr::opts_chunk$set(eval=FALSE, echo=TRUE,python.reticulate = FALSE)
```

KEY Concepts to review:

## Resume related technical content --\> JINER only

### Intern1 - Statistical Consultant (R, Tableau)

#### Short description of the project

I was mainly responsive for conducting a statistical analysis research project on analyzing client behavior in The Connection. And The Connection is a statewide community-based human services agency that helps clients in their programs to solve problems of homeless, mental illness, substance use, and community justice rehabilitation. The main goal of my project was to analyze social and psychological factors associated with the likelihood of behavior incidents during clients' participation in the Connection programming and also investigate what kinds of characteristics do they share in common and where do these clients locate after the programs.

#### What techniques did you use for this project?

First, I performed data wrangling and data blending on about 10 data sets related to client behaviors or criminal incidents and designed 9 key metrics to evaluate clients' social and psychological status, such as suicidal or homicidal risks, trauma history, or involvement and disruption from alcohol and drug use.

We also used multiple regression analyses to asses the relationship between clients' incident score (which is a composite measure of their incident frequency and severity) and their social, psychological risk factors, while controlling for demographic factors.

The latent class analysis was also conducted for client profiling and identify three groups with different level of risks, where each client was assigned to a class based on the similarity of their pattern of response on the classification variables to other clients ---\>

-   data manipulation & visualization methods / packages

-   multiple regression model

-   latent class analysis / k-means clustering

-   chi-square (goodness of fit, contigency)

### Intern2 - Data Science Intern (R, Python)

#### how to design key metrics?

#### ETL pipeline and R ShinyApp dashboard

references: [Extract Transform Load (ETL)](https://www.databricks.com/glossary/extract-transform-load) \| [Dashboards in R Shiny](https://www.r-bloggers.com/2022/03/dashboards-in-r-shiny/)

-   **`ETL Pipeline:`** ETL stands for **`"Extract, Transform, Load"`** so it mainly includes these three independent processes of data integration to pull data from one database and move it to another. Once loaded, data can be used for reporting, analysis, and deriving actionable business insights.

-   **`R Shiny Dashboard:`** let you access complete web application framework within R environment. Easily turn your work in R including analyses, visualizations, machine learning models, and more into web applications that deliver value to business.

    -   It also enables **easy `customization of dashboard`** using custom HTML, CSS, Javascript and so on to create a unique, branded dashboard that's not possible with other BI software suite, where we can add colors, logos, fonts and more to better represent our business.

    -   It is also **`open source and cost-friendly`** compared to its counterparts like Power BI or Tableau.

#### User-based Collaborative Filtering algorthm

-   **`User based collaborative filtering algorithm`** finds similar users and gives them recommendations based on what other people with similar consumption patterns like to purchase as well.

-   While **`Item-based Collaborative Filtering`** finds similarity patterns between items themselves and recommends them to users based on the computed information.

-   In my internship for example, the goal of implementing this method was to decide what other courses we could recommend users to choose. Especially because our targeted customers are parents who have kids that are between 3-7 years old, and they usually purchase a general course bundles where they could decide later if they wanna let their kids learn courses within different categories such as sports, arts, and natural sciences, for example. In order to motivate them to consume or user those course credits within the bundle so that our business could actually make revenues or profits because our customers can always request a refund at anytime if they do not want to continue using these credits. In this case, the User-based Collaborative Filtering algorithm allows me to find parents whose kids might have similar interests or characteristics, and then we could recommend courses from other departments to our potential customers within the same customer segment based on the similarity of these customers.

#### RFM analysis and k-means

-   RFM stands for **`Recency, Frequency, and Montary Analysis`**. It is an effective customer segmentation technique to help us make strategic choices in business, and in my internship example, it would be used to formulated targeted customer retention strategies based on different customer segments.

-   RFM essentially distinguishes and segments customers into similar clusters and targets them with separated and personalized promoting methodologies, which in turn makes strides in customer engagement and retention.

-   RFM metrics are very important to understand the behavior of customers as frequency and monetary value affects a customer's lifetime value, and recency affects retention, which is a measure of engagement. I used a hybrid of RFM and K-Means clustering to investigate who are the best customers, who contribute to the churn rate etc. and cluster out customers into three main classes based on the level of priority our customer management team should focus on either maintaining this customer, or activating this customer if they had been relatively less responsive group.

-   **`K-means`** is an iterative algorithm that tries to partition the dataset into K distinct clusters, in this case our primary customer information datasets which include when they start and quit using our services, how long and how often they use our services, etc.

    -   K-means tries to make the intra-cluster data points as similar as possible while also keeping the clusters as far as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster's centroid is at the minimum.

        -   The fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which data may be clustered. I used two methods to decide K value for the K-means clustering. One is the Elbow method and another is silhouette score.

            -   **`The Elbow Method i`**s a heuristic used in determining such optimal number of clusters. It is one of the most popular methods where we could simply plot the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use.

            -   **`The silhoutte analysis`** is an alternative. It is computed as the difference between the mean distance to the points in the nearest cluster that data point is not a part of and the mean intra-cluster distance to all the points in its own cluster, divided by the maximum value between these two.

                -   Silhoutte coeffcient is between-1 and 1, where 1 denotes the best meaning that data is very compact within cluster to which it belongs and far away from the other clusters.

            -   The major difference between elbow and silhoutte is that elbow only calculates the euclidean distance whereas silhoutte takes into account variables such as variance, skewness, high-low differences, etc.

            -   Elbow is better for datasets with smaller size or time complexity because of its calculation simplicity.

#### classification --\> predict user churn rate

### Intern3 - Data Science Intern

#### SQL (pull data), SQL templates

#### Feature Engineering

references: [Data Science in 5 minutes: What is One-hot Encoding?](https://www.educative.io/blog/one-hot-encoding)

[What is feature engineering by Harshil Patel](https://towardsdatascience.com/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10)

[Fundamental Techniques of Feature Engineering for Machine Learning by Emre Rençberoğlu](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114#3abe)

1.  ***What is feature engineering?***

    Feature engineering is the process of selecting, manipulating. and transforming raw data into desired features that can be used in statistical or machine learning approaches (e.g. supervised learning --- predictive models).

2.  ***What is the goal of feature engineering?***

    To simplify and speed up data transformations while also enhance model accuracy. becuz in raw data in real-life scenarios are often quite complex, we may have data type like texts, images, hyperlinks, etc. And regardless of the data or architecture, a terrible feature will have a direct impact on our model performance.

3.  ***What are the processes of feature engineering?***

    -   **`Feature Creation:`** which creates features involving creating new variables which will be most helpful for our model. This can be adding, removing, or aggregating some features.

    -   **`Transformation:`** which is simply a function that transforms features from one representation to another without changing the actual values it meant to represent.

    -   **`Feature Extraction:`** the process of extracting features from a data set to identify useful information without distorting the original relationships or significant information. This process compresses the amount of data into manageable quantities for our algorithms to process.

        -   [12 Dimensionality Reduction Techniques with Python codes](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/)

        -   [Feature Extraction using Principal Component Analysis by Kai Zhao](https://towardsdatascience.com/feature-extraction-using-principal-component-analysis-a-simplified-visual-demo-e5592ced100a#:~:text=PCA%20is%20a%20dimensionality%20reduction,understand%20the%20final%20two%20steps.)

        -   [LDA vs. PCA](https://towardsai.net/p/data-science/lda-vs-pca#:~:text=LDA%20focuses%20on%20finding%20a,variation%20in%20the%20data%20set.)

    -   **`Exploratory Data Analysis:`** EDA is a powerful and simple tool to improve our understanding of data by exploring its properties. The goal may be to create new hypotheses or find patterns in data.

        -   see some EDA techniques and sample code in python [here](https://www.projectpro.io/article/exploratory-data-analysis-in-python-stop-drop-and-explore/427).

    -   **`Benchmark:`** a benchmark model is the most user-friendly, dependable, transparent, and interpretable model against which we can measure our own. It is a good idea to run test datasets to see if our new machine learning model outperforms a recognised benchmark. These benchmarks are often used as measures for comparing the performance between different machine learning models like neural networks and support vector machines, linear and non-linear classifiers, or different approaches like bagging and boosting.

4.  ***What are the feature engineering techniques?***

    -   **`Imputation:`** to deal with missing values coming from human errors, data flow interruptions, privacy concerns, or other factors contributing to missing values.

        -   **`kNN Imputation:`** (k-nearest neighbor algorithm) ---\> matching a point with its closet k neighbors in a multi-dimentional space.

            -   see kNN example in Python [here](https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/).

            -   [The use of KNN for missing values by Yohan Obadia](https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637#:~:text=KNN%20is%20an%20algorithm%20that,all%20kind%20of%20missing%20data.)

        -   **`Drop entire rows or columns:`** we could use 70% as an example value and try to drop the rows and columns where missing values are higher than this threshold.

            ```{python}
            threshold = 0.7

            # drop columns w/ missing value > threshold
            data = data[data.columns[data.isnull().mean() < threshold]]

            # drop rows w/ missing value > threshold
            data = data.loc[data.isnull().mean(axis=1) < threshold]
            ```

        -   **`Numerical Imputation:`** fill out missing values with 0 which represents NA. OR, to use the medians instead, becuz as the averages of columns are sensitive to outliers, while medians are more solid in this respect.

            ```{python}
            # Filling all missing values with 0
            data = data.fillna(0)

            # Filling missing values with medians of the columns
            data = data.fillna(data.median())
            ```

        -   **`Categorical Imputation:`** replace missing values with the highest value in the column. In some cases, if we believe values in this column are evenly distributed and there is NO dominating value, then imputing a category like "other" would be a better choice.

            ```{python}
            #Max fill function for categorical columns
            data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True)
            ```

    -   **`Handling Outliers:`** to remove outliers from dataset so that we can produce a more accurate data representation. Depending on the model, this effect of outliers could be large or minimal (e,g, linear regression is particularly susceptible to outliers so we need to handle them prior to model training)

        1.  **`Removal:`** DELETE outiler completely from the distribution. However, if there are outliers across numerous variables, this strategy may result in a big chunk of data being missed,

        2.  **`Replacing values:`** with suitable imputation

        3.  **`Capping:`** use an arbitrary value or a value from a variable distribution to replace the maximum and minimum values.

            ```{python}
            # Capping outlier rows with Percentiles
            upper_lim = data['column'].quantile(.95)
            lower_lim = data['column'].quantile(.05)

            data.loc[(df[column] > upper_lim), column] = upper_lim
            data.loc[(df[column] < lower_lim), column] = lower_lim
            ```

        4.  **`Discretization:`** convert continuous variable into discrete ones by constructing a series of continuous intervals (or bins) that span the range of our desired variable.

            ```{python}
            # Numerical Binning Example
            VALUE      BIN
            0-30   ->  Low
            31-70  ->  Mid
            71-100 ->  High

            # Categorical Binning Example
            VALUE     BIN
            Spain  -> Europe
            Italy  -> Europe
            Brazil -> South America
            ```

    -   **`Log / BoxCox Transformation`**

        -   to turn a skewed distribution into a normal or less-skewed distribution

    -   **`One-hot Encoding:`** convert categorical variables such as gender, socio-economic status into a numerical form that can be processed by machine learning algorithms.

        -   In one-hot encoding, we could assign binary variable for categorical variables with two levels, and if it has more levels like race or weekdays, then we could convert these levels into ordered or non-ordered integers or numbers based on if there exists any order or relationship within these levels.

            ```{python}
            # one-hot encoding with pandas --> create dummy variables
            pd.get_dummies(df.dataframe, prefix='column_name')

            # one-hot encoding with Sklearn
            import sklearn.preprocessing as preprocessing
            targets = np.array['level1','level2','level3']

            labelEnc = preprocessing.LabelEncoder()

            new_target = labelEnc.fit_transform(targets)

            onehotEnc = preprocessing.OneHotEncoder()
            onhotEnc.fit(new_target.reshape(-1,1))
            targets_trans = onehotEnc.transform(new_target.reshape(-1,1))
            ```

    -   **`Feature Splitting:`** improve value of features toward the target to be learned

        -   *e.g. Data better contributes to the target function than Date and Time.*

            ```{python}
            df['Date and Time'] = pd.to_datetime(df['Date and Time'])
            df['Date'] = df['Date and Time'].dt.date
            ```

    -   **`Scaling:`** the scaling operation makes continuous features become similar in terms of range so that we could effectively save the model training time.

        1.  **`Normalization:`** where all values are scaled in a specified range between 0 and 1 via min-max normalization. This modification has no influence on the feature's distribution but it does exacerbate the effects of outliers due to lower standard deviations.

            ```{=tex}
            \begin{equation}
            \frac{\text{x - min(x)}}{\text{max(x) - min(x)}}
            \end{equation}
            ```
            ```{python}
            data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].max() - data['value'].min())
            ```

        2.  **`Standardization:`** (also known as z-score normalization) is the process of scaling values while accounting for standard deviation. To arrive at a distribution with a 0 mean and 1 variance, all data points are subtracted by their mean and the result divided by the distribution's variance. If the standard deviation of features differs, the range of those features will likewise differ.

            ```{=tex}
            \begin{equation}
            \frac{X - \mu}{\sigma}
            \end{equation}
            ```
            ```{python}
            data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()
            ```

5.  ***How do you evaluate model performances?***

6.  ***Explain k-fold cross validation.***

7.  ***Cross Validation in Time Series?***

8.  ***What is the difference between supervised and unsupervised learning?***

9.  ***What is feature?***

    A feature is any measurable input (variable of interests) that can be used in a predictive model. For example, in our consumer coupon prediction model, we may be interested in taking demographic, geographical, or environmental features into account such as the level of consumers income, the region or city they come from, and holidays etc.

### Capstone research

reference: [ARIMA by Adam Hayes](https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp) \| [An Introduction to Time Series Forecast](https://www.infoworld.com/article/3622246/an-introduction-to-time-series-forecasting.html) \| [GARCH](https://lost-stats.github.io/Time_Series/GARCH_Model.html)

#### time series forecasting (exponential smooth / ARIMA / GARCH)- differences, pros and cons

1.  ***What is time series forecasting?***

    To forecast or predict the future value or event over a period of time. It predicts future events by analyzing the trends of the past based on historical records. Time series forecasting is also an important area of machine learning and can be cast as a supervised learning problem.

2.  ***What is ARIMA?***

    ARIMA is called the autoregressive integrated moving average, It is a method for forecasting or predicting future outcomes based on historical time series. It is based on the statistical concept of serial correlation, where past data points influence future data points. *(The ARIMA model can also be understood as a form of regression analysis that gauges the strength of one dependent variable relative to other changing variables. Its goal is to predict future values by examining the differences between values in the series instead of through actual values.)*

    An ARIMA model can also be understood by each of the three components. The first one is autoregression, the AR part of the model that shows a changing variable that regresses on its own lagged or prior values. 2) The Integrated (I) part of the model represents the differencing of raw observations to allow for time series to become stationary (i.e. in this process, data values are replaced by the difference between data values and previous values). 3) The last part, moving average, the MA part of the model incorporates the dependency between an observation and a residual error from a moving average model applied to lagged observations.

3.  ***What is SARIMA?***

    For this research, I adjusted the algorithm to either select an ARIMA or a SARIMA model based on if there exists any seasonal considerations we need to account for. The SARIMA, which is a seasonal ARIMA model, extends ARIMA by adding a linear combination of seasonal past values and/or forecast errors.

4.  ***What are the differences between Autoregressive and Moving Average models?***

    So an autoregressive model with a lag order of 1, for example, is a process where current value is based on its immediately preceding value like yesterday, while in an AR model with a lag order of 2, the current value is based on the previous two values like both yesterday and the day before yesterday.

    A moving average, though, is a calculation used to analyze data points by creating a series of averages of different subsets from the full data set in order to smooth out the influence of outliers.

    ARIMA combines autoregressive features with those of moving averages so that it can take into account trends, cycles, seasonality, and other non-static types of data when making forecasts.

5.  ***What is GARCH?***

    GARCH stands for generalized autoregressive conditional heteroskedasticity, and it is a statistical model used to analyze some particular time series data when the variance error is believed to be serially autocorrelated. GARCH models assume that the variance of the error term also follows an autoregressive moving average process. In order words, we usually implement GARCH on the fitted ARIMA model. GARCH can be very useful to help predict the volatility of returns on financial assets. Volatility can be understood as how much and how quickly the value moves over a given span of time, for example, we can use price volatility to describe price fluctuations of a commodity.

6.  ***What are other time series forecasting techniques?***

    There are many other useful statistical and machine learning models for time series analysis. For example, regression models, exponential smoothing methods (which produce forecasts based on weighted averages of past observations, in other words, exponential smoothing produces forecasts where the forecast most closely resembles recent observations). Some useful popular exponential smoothing methods include the DES (double exponential smoothing) whose forecasts are weighted averages of both the trend and time series itself, as well as the Holt-Winter method which accounts for both trend and seasonality.

    Machine learning techniques like neural networks are also quite popular in time series analysis because they aim to solve problems that would be impossible or difficult to solve with these statistical or classical methods. For example, the recurrent neural networks (RNNs) were deigned to try remember important information about recent inputs so that they can be used to generate accurate forecasts. The LSTM (long short-term memory) network is a particular useful type of RNN in time series analysis. It basically has some forget gates and feed forward mechanisms that allow the network to retain information, forget non-important or not relevant inputs, and update the forecasting procedure to model and forecast more complex time series problems.

7.  ***What is the difference between exponential smoothing and ARIMA?***

    While exponential smoothing methods generates forecasts based on historical components of the data, ARIMA models take advantages of autocorrelation to produce forecasts. (Autocorrelation is when a time series displays correlation between time series and a lagged version of the time series, which simply means that current values also depend on previous values.)

#### time series clustering --\> Dynamic Time Warping, hierarchical clustering

#### k-medoids clustering (mixed data type)

#### test for stationarity (Augmented Dickey-Fuller test), normality (Shapiro Wilk), conditinal heteroscedasticity (ARCH LM), Box-Cox transformation

## SQL \| Python \| R \| Tableau

### Basic Statistics:

-   Confidence intervals

-   Tests of Hypotheses

-   T-tests, correlation, regression, analysis of variance, chi-square tests

## Probability: (usually test easy-medium prob questions)

-   `Probability spaces` as models for phenomena with statistical regularity

-   `Discrete spaces (binomial, hypergeometric, Poisson)`

-   `Continuos spaces (normal, exponential) and densities`

-   Random Variables \| Expectation \| Independence

-   Conditional probability

-   The Laws of large numbers

-   Central Limit Theorem (CLT)

***`MOST ASKED MATH TOPICS:`***

-   PROB:

    -   probability theorems

    -   sampling & experimental design

    -   random variables, PDF/CDF, expected values, etc.

-   STATS

    -   book: `Practical statistics for data scientists (Bruce)` \| engineering statistics handbook (NIST) \| Statistical inference (Casella & Berger)

    -   A/B Testing, hypothesis test

    -   differences of every type of distribution and how to test such type

    -   OLS and logistic

## ML Theory (for DS-Modeling job)

-   book: Introduction to Statistical Learning \| Elements of statistical learning \| Hands-on ML with Scikit Learn, Keras, and TensorFlow (by OREILLY)
-   bias variance \| sampling \| under/overfitting \| feature engineering \| model evaluation
-   how to select an apropriate model and features
-   pros and cons of different models
-   ETL pipeline/model, prod readiness

## A/B Testing and Product Sense

## Statistical Testing

-   parametric vs. non-parametric

    -   each method's assumptions and pros & cons

    -   differences: parametric --\> independent, unbiased sample \| non-parametric --\> ?

-   `case studies -- experiment design:`

    -   mean, standard deviation, median, IQR

    -   Hypothesis testing

        -   h0 / h1

        -   choose statistical test and significance level

        -   test statistics

    -   correlation (pearson / Spearman rank / Kendall rank)

    -   one-sample / two sample t-test \| ANOVA \| chi-square --\> goodness of fit, contigency

    -   test distribution

        -   histogram, skewness, kurtosis

        -   KS (Kolmogorov-Smirnov) test

        -   Shapiro-Wilk

        -   68-95-99.7

## Behavioral Questions
