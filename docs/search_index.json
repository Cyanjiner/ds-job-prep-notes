[["index.html", "DS Intern/NG Job Application Prep Notes Chapter 1 Summary resources for DS learning 1.1 Statistics, Probability, and A/B Testing 1.2 SQL / R / Python / Visualization Tools 1.3 Machine Learning resources 1.4 Interview Questions &amp; Resume", " DS Intern/NG Job Application Prep Notes Jiner Zheng 2022-09-16 Chapter 1 Summary resources for DS learning 1.1 Statistics, Probability, and A/B Testing 1.1.1 Stanford Courses (NON-EDUC): STATS 160-Introduction to Statistical Methods COVERS estimation | confidence intervals | test of hypotheses | t-tests | correlation &amp;&amp; regression | analysis of variance and chi-square tests STATS 116-Theory of Probability or CS109-Introduction to Probability for Computer Scientists COVERS probability spaces | discrete spaces (binomial, hypergeometric, Possion) | continuous spaces (normal, exponential) and densities | random variables | expectation | independence | conditional probability | the laws of large numbers and central limit theorem (CLT) 1.1.2 Online Resources Coursera: Introduction to Probability and Data with R by Duke University: If you are already familiar with R then skip week 3-5 sections, week 6 &amp; 8 are most useful for reviewing important probability concepts) Coursera: Six Sigma Advanced Define and Measure Phases: Week 3-5 covers probability &amp; statistics and statistical distribution Book: Practical Statistics for Data Scientists (available on Oreilly Stanford account has free access) Useful website for reviewing / practicing prob &amp; stats questions: Brilliant Udacity: Introduction to A/B Testing by Google Additional Tips: A/B Testing is NOT required by all DS/DA jobs, but if you are interested in applying for a Product Data Scientist then it is REQUIRED. Be sure to browse DS/DA interns JD so that you know what skills would be needed. 1.2 SQL / R / Python / Visualization Tools SQL syntax guide: SQL ZOO, W3schools SQL classes: Stanford class: CS145 Database Management and Data Systems Udemy: SQL-MySQL for Data Analytics and Business Intelligence + The Ultimate MySQL Bootcamp (should take less than a week to learn these two courses) Other resources: 18 BEST SQL online learning resources SQL Practices: Hackerrank (easy), Summary of SQL in LeetCode (go for medium and hard!!) MySQL instructions on Windows function &amp; Frame Clause, WITH common table expression (very useful) Udemy Python courses: Data Analysis with Pandas and Python + Python for Data Science and Machine Learning Bootcamp (tip: if you are familiar with Python then I recommend directly taking Andrew Ng’s ML courses and practice ur python ds coding thru hands-on projects and also regularly checking the Complete python data science cheatsheets) R: Datacamp (for learning how to program in R. However, most tech companies prefer Python so no need to be an R expert, but it is good to learn especially if you are also interested in doing research in academic) Tableau / Power BI: Learning Path: Your Guide to become a Tableau Expert | Tableau Tutorial | Power BI Tutorial (looking at online tutorial guides are sufficient for learning data viz tools cuz they are easy, but if you prefer taking online video lessons then Coursera—Data Visualization and Communication with Tableau by Duke will be a good choice). Other helpful resources: The R Gallery (website that contains useful R visualizations examples and code) R Markdown Guide (should be helpful to learn Rmd too cuz it makes everything look pretty) Collection of RStudio Cheatsheets (a collection of cheat sheets for all common used data manipulation and visualization packages (e.g. ggplot2, dplyr, tidyr, stringr, lubridate, shiny, etc.) R Statistical Modeling Code Syntax Guide Python Data Science Cheat Sheet (Beginner) Python Data Science Cheat Sheet (PDF) (Collection of popular python libraries cheat sheets including Numpy, Pandas, Seaborn, Matplotlib, Scikit, SciPy - linear algrebra) Real Python Tutorials 1.3 Machine Learning resources 1.3.1 Stanford Classes: STATS 202-Data Mining and Analysis (Terms: Aut, Sum) –&gt; course website STATS 216-Introduction to Statistical Learning (Terms: Win) –&gt; course syllabus: This is a math-light version of STATS 202 CS 129-Applied Machine Learning (Terms: Win) –&gt; course website: similar to the ML course by Andrew Ng on Coursera CS 229-Machine Learning (Terms: Aut, Win, Spr, Sum) –&gt; course website: This is a very MATHY ML class, so if you are not comfortable with doing mathematical proofs of some ML theories do not take it for credit. USEFUL RESOURCE: A collection/guide of Stanford AI courses under CS/STATS department | Stanford Grades Distribution 2020 1.3.2 Online Resources Machine Learning Specialization by Andrew Ng on Coursera Topics covered: Supervised Learning: multiple linear regression, logistic regression, neural networks, &amp; decision trees) Unsupervised Learning: clustering, dimensionality reduction, recommender systems Some AI &amp; ML innovation: evaluating and tuning models, taking a data-centric approach to improving performance Applied Learning Project: Build ML models using Numpy &amp; Scikit Build &amp; train a neural network with Tensorflow to perform multi-class classificaton Build &amp; use decision trees and tree ensemble methods, including forest and boosted trees Build recommender systems with a collaborative filtering approach and a content-based deep learning method 15 hours of expert ML videos. 《An Introduction to Statistical Learning》(This book is also used for Stanford’s course STATS 202: Data Mining and Analysis) Machine Learning 101 on Towards Data Science and many other articles Other useful resources (notes): Data Science Specialization Course Notes (Notes for all 9 courses in Coursera Data Science Specialization from JHU, taken by Xing —&gt; topics include Experimental Design / EDA / Statistical Inferences / Regression models / Practical Machine Learning, etc.) Notes on《Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow》 (GitHub repo for notes &amp; code [.ipynb] on the book w/ same name) This book is also available on Oreilly Data Science with R: A Resource Compendium very very very complete collection of data science resources with R by Martin Monkman, ranging from topics like data wrangling, Bayesian methods, to time series modeling &amp; ML methods) Natural Language Processing Notes – Python (chapters from Python Notes for Linguistics by Alvin Chen). Additional Tips: All resources listed as other useful resources are mainly for your references when you need to actually implement certain methods / conduct a project / or to review certain syntax or concept. I personally DO NOT recommend beginners to start their learning journey with these resources, because it is much more important that you have already built a SOLID foundation in all fields mentioned above through SYSTEMATIC learning processes. 1.4 Interview Questions &amp; Resume MIT Sample Resumes VMock Dashboard (A smart platform that rates &amp; analyzes your resume) FAANGPath Resume Template (a FREE tech resume template built with LaTeX on Overleaf) DS Interview related GitHub respository: Data-Science-Interview-Resources | 120-Data-Science-Interview-Questions Cracking-the-data-science-interview(this one contains almost ALL related resources for DS job prepraration, but it might be overwhelming if you just start your DS-prep journey [a lot of cheat sheets on KEY topics/concepts, so I recommend selectively using some resources there). "],["statistics-and-probability.html", "Chapter 2 Statistics and Probability 2.1 IMPORTANT Concepts to review 2.2 Designing Studies", " Chapter 2 Statistics and Probability 2.1 IMPORTANT Concepts to review Probability Basics and Random Variables beginnings of prob: sample spaces, basic counting and combinatorial principles (not necessary to know all ins-and-outs but helpful to understand basics for simplifying problems random variables expectation variance covariance Probability Distributions discrete &amp; continuous uniform, normal, poisson, binomial, geometric Hypothesis Testing central limit theorem sampling distributions p-values confidence intervals type I and type II errors Modeling maximum likelihood estimation bayesian statistics 2.1.1 Key terms for Variability Metrics Variability (also dispersion) measures whether the data values are tightly clustered or spread out. Deviations / Errors / Residuals: the difference between the observed values and the estimate of location (i.e. mean / median, etc.) Variance / mean-squared-error: the sum of squared residuals from the mean divided by n-1 where n is the number of data values. Standard deviation: the square root of variance Mean absolute deviation: the mean of the absolute values of the deviations from the mean. Percentile / Quantile: the value such that P percent of the values take on this value or less and (100-P) percent take on this value or more. Interquartile range / IQR: the difference between the 75th percentile and the 25th percentile. 2.2 Designing Studies Population: A collection of individuals or objects that we will be analyzing on their properties. Sample: A representative subset of population chosen to be analyzed (a well-chosen sample contains most of the information about a particular population parameter. 2.2.1 Identifying variables type: Numerical vs. Categorical numerical —&gt; continuous or discrete? (based on whether or not they can take on an infinite number of values or only non-negative whole numbers, respectively) categorical —&gt; ordinal? (whether or not levels have a natural ordering) Associated (Explanatory) vs. Independent (Response) —&gt; show relationships w/ other vars? Confounding variables 2.2.2 Classify study type as ovbservational or experimental Observational studies: researcher collects data by observing but not directly interfering with how data arise —&gt; correlation only Retrospective study: when an observational study uses data from the past Prospective study: …. data are collected throughout the study Experiments: when researchers randomly assign subjects to treatments (can be causal) 2.2.3 Sampling Techniques Probability Sampling Random sampling: choosing sample randomly without any given logic —&gt; each member has an equal chance of being selected in the sample. Stratified sampling: First divide population into homogenous strata (subjects within each stratum are similar but different across strata), then randomly sample from within each strata. e.g. to make sure both genders are equally represented in a study, we might divide the population into males and females and then randomly sample from within each gender group Cluster sampling: Divide population into heterogenous clusters (subjects within clusters are different but clusters are similar to each other —&gt; randomly sample a few clusters Multistage sampling add one other step to cluster sampling: randomly sample observations from WITHIN each cluster Systematic sampling Non-Probability Sampling Snowball Quota Judgement Convenience sample bias occurs when individuals who are easily accessible, are more likely to be included in the sample. Non-response bias happens when only a non-random proportion of the randomly sampled people respond to a survey —&gt; sample no longer representative (initial sample is random but the final valid sample is not) e.g. when we take a random sample of individuals from Stanford, but certain groups of population, such as from a lower socioeconomic status, are much less likely to respond to the survey —&gt; our sample is not representative enough of the entire Stanford community Volunteer Response bias occurs when sample consists of only people who volunteer to respond bcuz they have strong opinions on the issue (no initial random sample) 2.2.4 Principles of Experimental Design—Control, Randomize, Replicate, and Block—and their purposes Control — compare treatment of interest to a control group Randomize — randomly assign subjects to treatments Replicate — collect a sufficiently large sample, or replicate the entire study Block — block for variables known or suspected to affect outcome if there are variables known or suspected to affect the response variable, first group the subject into blocks based on these variables —&gt; then randomized cases within each block to treatment groups e.g. design an experiment to investigate if energy gels make you run faster: the treatment group gets the energy gel, the control group does not. It is suspected that energy gels might effect pro and amateur athletes differently therefore we block for pro status. &lt;-- we divide our sample into pro and amateur athletes, then randomly assign pro and amateur athletes to treatment and control groups so that both pro and amateur athletes are equally represented in the resulting treatment and control group. Blocking variable vs. Explanatory variable Explanatory variables (factors) are conditions we impose on our experimental units. Blocking variables are characteristics that the experimental units come with (which may affect how experimental units respond to response variable differently). Other terminologies Placebo: a fake treatment, often used as the control group in medical studies Placebo effect: when experimental units show improvement just becuz they believe they’re receiving a special treatment Blinding: when experimental units DO NOT know they are in the control or treatment groups. Double-blind study: when BOTH the experimenters and researchers DO NOT know who is in the control or treatment group. Experimental Design Workflow: Control any possible confounders / confounding variables (non-explanatory factors that may influence different responses) Randomize into treatment and control groups Replicate by using a sufficiently large sample or repeating the experiment Block any variables that might influence the response * Stratified sampling allows for controlling for possible confounders in the sampling stage, while blocking allows for controlling for such variables during random assignment. 2.2.5 Random Sampling vs. Random Assignment If random sampling has been employed in data collection, the results should be generalizable to the target population. (but still NOT causal) WHY? —&gt; if subjects are randomly selected from the population, then each subject in the population is equally likely to be selected so that the resulting sample is likely representative of the population. If random assignment has been employed in study design, the results suggest causality. WHY? —&gt; in our sample, subjects usually exhibit slightly different characteristics from one another. Through random assignment, we ensure that these different characteristics are represented equally in the treatment and control groups —&gt; allows us to attribute any observed difference between treatment and control groups to treatment being observed on the subjects, since otherwise these groups are essentially THE SAME. A study that relies on volunteers employ random assignment (experiment), but NOT random sampling can be used to make causal conclusions but ONLY apply to the sample (so results cannot be generalized). A study that uses NO random assignment, but DOES use random sampling, is a typical observation study. Results can ONLY be used to make correlation statements, but they CAN be generalized to the population at large. A study that DOES NOT use random assignment or random sampling, can ONLY be used to make correlational statements, and these conclusions are NOT generalizable. This is an unideal observational study. 2.2.6 Hypothesis Tests and Resampling 2.2.7 Statistical Significance and p-values p-value: Given a chance model that embodies the null hypothesis, the p-value is the probability of obtaining results as unsusual or extreme as the observed results. Alpha: The probability threshold of \"unusualness\" that chance results must surpass for actual outcomes to be deemed statistically significant. Type 1 error (false-positive): Mistakenly concluding an effect is real (when it is due to chance). (i.e. reject H0 when it is actually true) Type 2 error (false-negative): Mistakenly concluding an effect is due to chance (when it is real). (i.e. fails to reject H0 when it is actually false) "],["ab-testing.html", "Chapter 3 A/B Testing 3.1 What is A/B testing? 3.2 What you can’t do with A/B testing? 3.3 Defining the hypothesis 3.4 Defining Metrics and Gathering Data 3.5 Designing an A/B test 3.6 Step1: Choose and characterize metrics for both sanity check and evaluation 3.7 Step 2: Choose significance level, statistical power and practical significance level 3.8 Step 3: Calculate required sample size 3.9 Step 4: Take sample for control/treatment groups and run the test 3.10 Step 5: Analyze the results and draw conclusions 3.11 Other things to keep in mind", " Chapter 3 A/B Testing Experimentation ranging from the underlying infrastructure, to design metrics and dashboards, to running and analyzing experiments to the processing culture needed to facilitate data-driven solutions 3.1 What is A/B testing? A/B testing is a general methodology used online when you wanna test out a new product or a feature. And what you’re doing is you are going to take two sets of users and you will show one set, a control set, your existing product or feature, and then another set, your experiment, the new version. Then, you will examine how did these users from two groups respond differently in order to determine which version of your feature is better. Example: It can be a novel big change or addition in product or feature like when Amazon initially decided to launch their first personalized product recommendations feature based on A/B testing results that showed a huge increase in revenue by adding this new feature. It can also be a very small or trivial feature such as when Google tested 41 different shades of blue. The key thing in A/B testing is that you have a consistent response from your control and experiment group so that you can actually determine and structure the experiment to determine whether there is a significant behavior change in your experiment group. The goal in A/B testing is to design an experiment that is gonna be robust and give you repeatable results so that you can actually make a good decision about whether or not to actually launch that product or feature. A/B Testing compared to hypothesis testing in traditional fields of study: In traditional fields of study such as clinical trials in psychology or medical research, we know a lot of information about participants such as their demographics, and we usually have a relatively small number of participants. However, when testing an online product, we might have millions of users, hundreds or thousands of clicks, etc., and we DO NOT know much about who is taking those actions on the other end. 3.2 What you can’t do with A/B testing? Case 1: New Experiences (e.g. premium service that offers additional functionality) (p.s. to access premium, user need to upgrade, create a log-in, and explore new functionality) A/B testing isn’t useful when you wanna test out new experiences. Because when you are testing a new experience, you have an existing set of users and they might feel that you have changed their experience and they preferred their old way (and this is change aversion). The other case is that they can be really like this new experience and they test out everything (this is called a novelty effect). What happens in a new experience are two issues: we are not sure about the baseline for comparison we can’t control the exact time we need in order to actually have our users adapt to the new experience. So that we can’t know the plateaued experience so that we can actually make a robust decision. Case 2: Time-dependent/related cases (e.g. Referral) For example, if we have a website that recommends apartment rentals, but people don’t look for apartments that often, what you really want is to grow your business by referrals to other people who like your service. The main issues in this case are: The scope of an experiment will be really hard to measure whether people actually come back to you from more referrals or from other reasons. We can’t estimate the time that people actually spend in making the action of referral happen. Cases 3: Missing/Complete content or services A/B testing can’t really tell us if we are missing something. For example, if we are building a song or a book reviews website, A/B testing can’t tell us if we are missing the entire other book that we should be reviewing but we aren’t reviewing at all. Other Techniques to complement A/B testing: Qualitative research: A/B testing can give you a lot of broad quantitative data, but other techniques give you very deep and qualitative data that are really complimentary to A/B testing User experience research Focus groups and surveys Human evaluations Quantitative approach: We can also analyze the logs of what users did on the website to see if a hypothesis can be developed about what’s causing changes in their behavior. And that’s something where you may want to go forward and actually design and randomize an experiment through a perspective analysis. We can use the two data sources to compliment each other. 3.3 Defining the hypothesis Initial Hypothesis: changing the “start now” button from orange to pink will increase how many students explore Audacity’s courses. Choosing Metrics: BAD CHOICES: Total # of courses completed —&gt; time issue (not should about the actual duration to complete course) # of clicks —&gt; percentage issue (e.g. 2/12 clicks in control group vs. 1/3 clicks in experiment group: # of clicks in experiment is \\(\\downarrow\\), but its ratio is actually \\(\\uparrow\\) GOOD CHOICES: \\[\\begin{equation} \\text{CTR (click-through-rate)} = \\frac{\\text{Number of clicks}}{\\text{Number of Page views}} \\end{equation}\\] \\[\\begin{equation} \\text{Click-through probability)} = \\frac{\\text{Unique visitors who click}}{\\text{Unique visitors to page}} \\end{equation}\\] Updated hypothesis: Changing the “Start Now” button from orange to pink will increase the click-through-probability of the button. (--&gt; assume that this will ultimately increase business profits). 3.4 Defining Metrics and Gathering Data 3.4.1 High-Level concepts for metrics define business objectives break overall objectives down to step by step using customer funnel Funnel –&gt; we typically have fewer and fewer users that get to each stage of the funnel OEC (overall evaluation criterion) —&gt; a composite metric / an objective weighted function that combines all of these different metrics Categories of SUMMARY Metrics sums and counts means, medians, percentiles probability (0 or 1) and rate (0 or more) ratios 3.4.2 Methods for Coming up w/ Proxy Metrics or Validating Metrics Retrospective Analysis: if we have logs or other data capture mechanisms to see what users do. Running analyses on this existing set of observational data without an experiment structure is called retrospective analysis or observational analysis. [correlation NOT causation] Long-term Prospective Experiments Human Evaluation 1. It is important to take into account your corporate culture as you define business metrics becuz some companies care more about obtaining market share / making revenues, but others just wanna make their existing users happy 2. It is usually more robust to triangulate between different methods. For example, if we wanted to see if students are really happy with our course website, we might look through our logs and say that somebody took a second course, which we’ll say is being happy: 1) what did they do? 2) how long did they spend? 3) how many months were they active for this site? —&gt; we wanna get some baseline for that. And then given that they took a second course, then we might also want to trigger surveys that happened within your site or do a focus group where people who’ve done a single course actually get a survey that says are you considering taking a second course? 3.4.3 Gathering Additional Data External data: see what data is OUT relating to similar websites / mobile applications User Experience Research (UER): tracks a specific user experience and understand a typical user journey in terms of experience good for brainstorming ideas from coming up with ideas to changes to test / to identifying problems with your user experience in a way that you can translate into a possible metric that you could use to evaluate your A/B test. can use special equipment such as an eye-tracking cameras to see what users are looking at even if they don’t click on the button want to validate results of UER with something like the retrospective analyses Focus Groups: where you bring a bunch of users or potential users together for a group discussion to elicit feedback. You can talk to more total users than with a UER study. but you can’t go as deep for each person. get feedback on hypotheticals run the risk of group think and convergence on fewer opinions Surveys: where u recruit a bunch of people to ask them questions, either online, or in person, or via telephone cheap for getting almost all users involved data often quantitative but not very deep or individually customized useful for metrics you cannot directly measure need to be careful about survey results —&gt; bcuz users don’t have to tell the truth and their answers can be dependent on how the questions are phrased. Example: Which techniques to use? Measure user engagement (course completion too long-term) Survey | UER + Retrospective analysis (of users who have completed courses and see what behaviors they have in common) Decide whether to extend inventory Focus group —&gt;get ideas from users about what products they wanna see External data —&gt; what users buy from other shopping sites Which ads get most views External data —&gt; look for studies to see if there’s something we can measure like time spent on the page / mouse hover events —&gt; use as a proxy for whether the ad was viewed UER —&gt; to observe users and see what ads they are paying attention to with an eye-tracking camera and then try to find a metric that correlates with that 3.4.4 Segmenting and Filtering Data External reasons: We want to filter out abuse on our site such as spam or fraud (e.g. competitor visiting) Internal reasons: when our change only impacts a subset of the traffic GOAL: to de-bias or dull-bias our data 3.5 Designing an A/B test reference: 1. A Summary of Udacity A/B Testing Course by Kelly Peng The A’s and B’s of A/B Testing by Tanmayee W 3.5.1 Summary workflow of A/B testing: Choose and characterize metrics to evaluate experiments —&gt; identify what we care about and how we wanna measure the effect Choose significance level (alpha), statistical power (1-beta) and practical significance level —&gt; we rly wanna launch the change if tests are statistically significant Calculate required sample size Take sample for control or treatment groups and run the test Analyze results and draw valid conclusions 3.6 Step1: Choose and characterize metrics for both sanity check and evaluation Invariant metrics: metrics we choose for sanity check these metrics should NOT change across control and experiment groups during the course of the experiment they are NOT supposed to be affected by the experiment if they DO change then there is something fundamentally wrong in the experiment setup. Evaluation metrics: metrics used to measure which variation is better e.g. we could use daily active users (DAU) to measure user engagement e.g. use click through rate (CLR) to measure a button design on a webpage Four Categories of Metrics to keep in mind Sums &amp; counts Distributional metrics (mean | median | percentiles) Probability &amp; rates (e.g. click-through probability, CLR) Ratios: any two numbers divide by each other (e.g. # of revenue-generating clicks / total # of clicks) Sensitivity and Robustness to consider we want to choose a metric that has high sensitivity —&gt; metric can pick up the change we care about we want to choose metric to be robust against changes we DO NOT care about —&gt; metric does not change a lot when nothing we’re interested happened Measure Sensitivity &amp; Robustness Run experiments Use A/A test to see if metrics pick up difference (if yes, then metrics are NOT robust) Retrospective analysis 3.7 Step 2: Choose significance level, statistical power and practical significance level Usually the significance level is 0.05 and power is set as 0.8. Practical significance level varies depends on each individual tests —&gt; tells us how much change the test detscts that makes we rly wanna launch the change. It is important to understand what constitutes as statistically significant change may NOT be practically significant change! Business needs have to be identified and ROI (return on investment) has to calculate if the change has to be actually rolled out. Is the roll-out worth the efforts? | Is business impact greater if we decide to launch the change? While we make these critical decisions, it may be worthwhile to consider opportunity costs (the loss of potential gain from other alternatives); engineering and roll-out costs (costs to launch the new product or services); customer support or sales issues that may be involved if we go ahead with the change. 3.8 Step 3: Calculate required sample size In this step, we would need to consider the choice of metric | choice of unit of diversion | choice of population into account bcuz they ALL affect the variability of our metrics. Then decide on the size of experiment. Subject for a/b testing is commonly called Unit of Diversion event-based (e.g. page views) —&gt; useful for measuring latency change such as website load time anonymous id-based (e.g. cookies) —&gt; user visible changes user-id based —&gt; user visible changes Population depends on what subjects are relevant for the study under consideration. Is the population of particular demographic, region/country relevant? Choose accordingly. How to reduce the size of an experiment to get it done faster? increase significance level alpha reduce power (1-beta) which means increase beta change the unit of diversion if originally it is not the same with unit of analysis unit of analysis: denominator of our evaluation metric 3.9 Step 4: Take sample for control/treatment groups and run the test Things to keep in mind: Duration: What’s the best time to run it and how long the experiment would take place? Students going back to college? Holidays? Weekend vs. weekdays? Exposure: What fraction of traffic we want to expose the experiment to? Suggestion is to take a small fraction, run multiple tests at the same time (different days: weekend, weekday, holiday). Learning effect: When there’s a new change, in the beginning users may against the change (change aversion) or use the change a lot (novelty effect) But overtime, user behavior becomes stable and more fit to measure any effects of change, which is called plateau stage . the key thing to measure learning effect is TIME, but in reality we don’t have much luxury of taking that much time to make a decision. Suggestion: run a smaller group of users for a longer period of time 3.10 Step 5: Analyze the results and draw conclusions 3.10.1 First step — Sanity Check Check if our invariant metrics have changed. If sanity check failed then we do not proceed becuz it means there’s something wrong with experiment setup. —&gt; go analyze why sanity check has failed by using retrospective analysis or look into if there exists any learning effect. 3.10.2 Second step — Analyze the Results Case 1: One Single Metric —&gt; if NOT significant First try: break down the experiment into different segments check results on different platforms (web &amp; mobile) check results across different periods of time (days, weeks, months, years, etc.) Second try: cross checking by using different methods e.g. compare with non-parametric sign test (a test that compares the sizes of two groups &amp; it is non-parametric or “distribution free” which means it DOES NOT assume the data comes from a particular distribution) with parametric hypothesis test (tests that make assumptions about parameters of the population distribution which the sample was taken — e.g. Two-sample t-test, paired t-test, ANOVA, Pearson correlation). if two tests NOT agree —&gt; look into data critically cuz we might be suffering from Simpson's paradox (a phenomenon when a trend appears in different groups of data but it disappears when the different groups are combined and looked at holistically) Reasons for Simpson’s paradox: experiment setup is incorrect the change affects the new users and experienced users differently Case 2: Measuring Multiple Metrics at the same time One potential problem is that we might see a significant result by chance e.g. if we are running tests with 20 variants, and we test each hypothesis separately: P (one significant result) = 1 - P (no significant results) P (one significant result) = 1 - (1 - 0.05) ^20 = 0.64 Ways to Solve this problem: Bootstrap: divide the test samples into further more sample and run experiments again and again on each sub-divided sample —&gt; the significant metric should disappear if it occurred by chance in the first place. Bonferroni correction: Divide the significance level 0.05 by the number of tests run. Then check the statistical significance of our tests as against this adjusted level of significance. The problem of Bonferroni correction is it tends to be too conservative. If many metrics are tested at the same time, maybe none of them turned out to be significant. Control Family-wise Error Rate (FWER): Adjust the probability that any metric will show false positives Control false discovery rate (FDR): Fix an acceptable rate of false positives generated by any metric in the experiment. FDR = # false positives / # total rejections Say we decide 0.05 as FDR then this means that out of 100 samples, we are ready to accept 5 false positives per test. Another potential problem if — What if metrics are NOT moving at the SAME direction as we expected? e.g. expect DAU &amp; average length of time users use our app both increase. HOWEVER, we observe DAU decrease while average length of time increase. To solve this problem —&gt; dive deeper and figure out WHY Have one OEC (Overall Evaluation Criterion). A good OEC gives us a balance between short-term and long-term goal, or balance between different metrics. However, we need to keep in mind that OEC helps us understand what our business care about, and how do we balance metrics such as stay time and click, but it DOES NOT help us make a product change decision. 3.10.3 Last step — Draw Conclusions When we DO have a significant result from a test, we need to introspect a few things before deciding launch the change or not: Do we understand the change? What does the change mean to users and business stakeholders? Will the change actually bring about value for the business? How will our users feel post-launch? Will there be differentiated opinions among different customer segments? Do we risk losing some of our customers? In other words: do I have statistically significant and practically significant result in order to justify the change? Do I understand what the change actually done to our user experience? Is it worth it to launch? 3.11 Other things to keep in mind RAMP UP Always do a ramp up when we wanna launch a change after a/b testing. Becuzz we wanna know if there’s any incidental impact to unaffected users that we didn’t test in the original experiment. When we are ramping up the change, we may see the effect flatten out. Thus making the tested effect not repeatable. Reasons including: Seasonality Effect: Social network platform user behavior changes a lot when students start summer vacation or going back to school. Holidays affect users’ shopping behaviors a lot. Solution: use hold-back method —launch the change to everyone except for one small hold-back group of users, and continue comparing their behavior to the control group Novelty effect / Change Aversion: cohort analysis — involves breaking down dataset into related groups (or cohorts) over time and observing how their behavior changes. These groups or cohorts usually share common characteristics or experiences within a defined time-span. RISK: What risk are participants exposed to? The main threshold is whether the risk exceeds that of “minimal risk”. Minimal risk: defined as the probability and magnitude of harm that a participant would encounter in normal daily life. The harm considered encompasses physical, psychological and emotional, social, and economic concerns. If the risk exceeds minimal risk, then informed consent is required. BENEFIT: What’s the potential benefit of the outcome of the study? It is important to be able to state what benefit would be from completing the study. CHOICE: What other choices do participants have? In online experiments, the issues to consider are what the other alternative services that a user might have, and what the switching costs might be, in terms of time. money, information, etc. PRIVACY: What privacy do participants have? For new data being collected and stored, how sensitive is the data and what are the internal safeguards for handling that data? Then, for that data, how will it be used and how will participants’ data be protected? How are participants guaranteed that their data, which was collected for use in the study, will not be used for some other purposes? "],["database-management-and-data-systems-sql.html", "Chapter 4 Database Management and Data Systems (SQL) 4.1 CRUD Operations", " Chapter 4 Database Management and Data Systems (SQL) Database schemas show data types for each field (column) in all tables (data frame), and they also show relationships between tables. SQL (Structured Query Language) is used to answer questions / extract information both within and across relational database tables SQL Style Guide by Simon Holywell Keywords are reserved words for operations. 4.1 CRUD Operations 4.1.1 CREATE —&gt; Databases | Tables | Views | Users | Permissions | Security Groups /* create a new table w/ column names, data type, size */ CREATE TABLE test_table( -- unique table name test_date date, test_name varchar(20), test_int int ) /* data types: */ Dates: date (YYYY-MM-DD) | datetime (YYYY-MM-DD hh:mm:ss) | time Numeric: int | decimal | float | bit (1=TRUE, 0=FALSE. Also accepts NULL) Strings: char | varchar | nvarchar /* create a copy of an existing table */ CREATE TABLE new_table_name AS SELECT column1, column2, ... FROM existing_table_name, WEHRE conditions; 4.1.2 INSERT, — insert new records into existing database tables /* insert new columns &amp; values */ INSERT INTO table_name (col1, col2, col3) VALUES (&#39;value1&#39;, &#39;value2&#39;, &#39;value3&#39;) /* insert + select columns &amp; values from another existing table */ INSERT INTO table_name (col1, col2, col3) SELECT column1, column2, column3 FROM other_table WHERE condition(s); 4.1.3 UPDATE — Amend existing database records UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition(s) 4.1.4 DELETE — delete existing records from tables DELETE FROM table_name WHERE condition(s) /* another method: TRUNCATE --&gt; remove all data from ALL columns at once */ TRUNCATE TABLE table_name 4.1.5 Declare Variables —&gt; so it will be easier to use in later conditions without repetitively calling the same values -- declare your variables DECLARE @start DATE DECLARE @stop DATE DECLARE @affected INT; -- set relevant values for each variable SET @start = &#39;2022-01-01&#39; SET @stop = &#39;2022-12-31&#39; SET @affected = 5000; -- threshold for # of affected customers SELECT column1, column2, affected_customers FROM table_name WHERE date BETWEEN @start AND @stop AND affected_customers &gt;= @affected; 4.1.6 Temporary tables SELECT col1, col2, col3 INTO #my_temp_table -- #my_temp_table exists until connection or session ends FROM my_existing_table WHERE condition(s); -- Remove table manually DROP TABLE #my_temp_table 4.1.7 READ | VIEW Example: SELECT statements SELECT indicates which fields should be selected. FROM indicates where these fields are located. SELECT DISTINCT allows you to select field content. SELECT DISTINCT var_name1 AS new_var_name1, var_name2 FROM table_name; DROP DATABASE drops an existing SQL database ALTER DATABASE modifies a database ALTER TABLE modifies a table DROP TABLE table_name deletes a table CREATE INDEX creates an index (search key) DROP INDEX deletes an index BACKUP DATABASE is used in SQL Server to create a full back up of an existing SQL database TO DIST = 'filepath'; Use aliasing to rename columns: View is a virtual table that is the result of a saved SELECT statement. When accessed, views automatically update in response to updates in the underlying data. CREATE VIEW new_table_name AS SELECT id, var_name1, var_name2 FROM table_name; 4.1.8 GROUP BY | HAVING | WHERE GROUP BY splits data up into combinations of one or more values WHERE filters on row values HAVING appears after the GROUP BY clause and filters on groups or aggregates /* List the number of customers in each country, ordered by the country with the most customers first.*/ SELECT COUNT(customerID), country, FROM customers GROUP BY country ORDER BY COUNT(customerID) DESC; /* List the # of customers in each country, sorted high to low (but only include countries with more than 5 customers) */ SELECT COUNT(CustomerID), Country FROM Customers GROUP BY Country HAVING COUNT(CustomerID) &gt; 5 ORDER BY COUNT(CustomerID) DESC; /* List if the employees &quot;Davolio&quot; or &quot;Fuller&quot; have registered more than 25 orders */ SELECT Employees.LastName, COUNT(Orders.OrderID) AS NumberOfOrders FROM Orders INNER JOIN Employees ON Orders.EmployeeID = Employees.EmployeeID WHERE LastName = &#39;Davolio&#39; OR LastName = &#39;Fuller&#39; GROUP BY LastName HAVING COUNT(Orders.OrderID) &gt; 25; 4.1.9 JOIN examples /* INNER JOIN selects records that have matching values in BOTH tables */ SELECT column_name FROM table1 INNER JOIN table2 ON table1.column_name = table2.column_name; /* LEFT JOIN selects all records from the left table (table1) and the matching records from the right table (table2) */ SELECT column_name FROM table1 LEFT JOIN table2 ON table1.column_name = table2.column_name; /* RIGHT JOIN selects all records from the right table (table2) and the matching records from the left table (table1) */ SELECT column_name FROM table1 RIGHT JOIN table2 ON table1.column_name = table2.column_name; /* FULL OUTER JOIN returns all records when there is a match in left (table1) OR right (table2) table records -- FULL OUTER JOIN == FULL JOIN */ SELECT column_name FROM table1 FULL OUTER JOIN table2 ON table1.column_name = table2.column_name WHERE condition; /* a self join is a regular join, but the table is joined with itself */ SELECT column_name FROM table1 T1, table2 T2 -- T1, T2 are two aliases of table 1 and table 2 WHERE condition /* SQL Self Join Example: matching customers from the same city */ SELECT A.CustomerName AS CustomerName1, B.CustomerName AS CustomerName2, A.City FROM Customers A, Customers B WHERE A.CustomerID &lt;&gt; B.CustomerID -- &lt;&gt; stands for not equal AND A.City = B.City ORDER BY A.City; 4.1.10 UNION Operator UNION discards duplicates while UNION ALL does not the UNION operator is used to combine the result-set of two or more SELECT statements Every SELECT statement within UNION must have the same number of columns The columns must also have similar data types The columns in every SELECT statement must also be in the same order /* UNION : selects only distinct values by default */ SELECT column_name(s) FROM table1 UNION SELECT column_name(s) FROM table2; /* UNION ALL : allows duplicate values */ SELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2; /* example1: returns the German cities (duplicate values also) from both the customers and the suppliers table */ SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION ALL SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; ORDER BY City; 4.1.11 CASE statements can be used to create columns (new variables) for categorizing data filtering data aggregating data based on results of a logical test e.g. COUNT(CASE statement)–&gt;returns the number of rows returned by case statements instead of string / text similar aggregations include SUM/AVG/ROUND(AVG(…), digits) Percentages with CASE and AVG SELECT column1, ROUND(AVG(CASE WHEN condition1 AND condition2 THEN 0 WHEN condition1 AND condition3 THEN 1 END), 2) AS pct_column, FROM table1 GROUP BY column1 SELECT id, home_goal, away_goal, CASE WHEN home_goal &gt; away_goal THEN &#39;Home Team Win&#39; WHEN home_goal &lt; away_goal THEN &#39;Away Team Win&#39; ELSE &#39;Tie&#39; END AS outcome /* ELSE NULL AS outcome --&gt; if everything else should be N.A. END AS outcome --&gt; if we want to exclude every other conditions END IS NOT NULL --&gt; keep all clauses excluding missing values*/ FROM match WHERE season = &#39;2013/2014&#39;; /* Example 1: CASE WHEN + COUNT GROUP BY */ -- Identify the home team as Bayern Munich, Schalke 04, or neither SELECT CASE WHEN hometeam_id = 10189 THEN &#39;FC Schalke 04&#39; WHEN hometeam_id = 9823 THEN &#39;FC Bayern Munich&#39; ELSE &#39;Other&#39; END AS home_team, COUNT(id) AS total_matches FROM matches_germany -- Group by the CASE statement alias GROUP BY home_team; /* Example 2: CASE WHEN + LEFT JOIN + WHERE */ SELECT m.date, t.team_long_name AS opponent, -- Complete the CASE statement with an alias CASE WHEN m.home_goal &gt; away_goal THEN &#39;Barcelona win!&#39; WHEN m.home_goal &lt; away_goal THEN &#39;Barcelona loss :(&#39; ELSE &#39;Tie&#39; END AS outcome FROM matches_spain AS m LEFT JOIN teams_spain AS t ON m.awayteam_id = t.team_api_id -- Filter for Barcelona as the home team WHERE m.hometeam_id = 8634; 4.1.12 TEXT operations SELECT colunm, LEN(column) AS column_length -- returns # of chars LEFT(column, 20) AS first_20_left_column -- returns first 20 chars from the LEFT RIGHT(column, 20) AS last_20_column -- returns last 20 chars from the RIGHT CHARINDEX(&#39;_&#39;, column) AS char_location -- returns the index of a char/string in column SUBSTRING(column, start, length) AS target_section -- returns substring starting at location 12 and has length of 12 FROM table; /* REPLACE */ SELECT TOP(5) REPLACE(column, &#39;a&#39;,&#39;b&#39;) AS replaced_column_with_b -- replaces char &#39;a&#39; in column with char &#39;b&#39; for all first 5 rows FROM table; "],["machine-learning-models.html", "Chapter 5 Machine Learning Models", " Chapter 5 Machine Learning Models "],["product-sense.html", "Chapter 6 Product Sense", " Chapter 6 Product Sense "],["my-ds-summer-intern-preparation-guide.html", "Chapter 7 My DS Summer Intern Preparation Guide 7.1 Resume related technical content –&gt; JINER only 7.2 SQL | Python | R | Tableau 7.3 Probability: (usually test easy-medium prob questions) 7.4 ML Theory (for DS-Modeling job) 7.5 A/B Testing and Product Sense 7.6 Statistical Testing 7.7 Behavioral Questions", " Chapter 7 My DS Summer Intern Preparation Guide KEY Concepts to review: 7.1 Resume related technical content –&gt; JINER only Intern experience 1 - Statistical Consultant (R, Tableau) data manipulation &amp; visualization methods / packages multiple regression model latent class analysis / k-means clustering chi-square (goodness of fit, contigency) Intern experience 2 - Data Science Intern (R, Python) how to design key metrics? ETL pipeline and R ShinyApp dashboard User-based Collaborative Filtering algorthm RFM analysis and k-means classification –&gt; predict user churn rate Intern experience 3 - Data Science Intern SQL (pull data), SQL templates feature engineering sampling (distribution / weighted sample) one-hot encoding | standardization (min-max) | dummy coding | missing values imputations (kNN) log / exp / box-cox PCA | LDA ETL pipeline –&gt; model evaluation and report (k-fold cross validation) Capstone research time series forecasting (exponential smooth / ARIMA / GARCH)- differences, pros and cons time series clustering –&gt; Dynamic Time Warping, hierarchical clustering k-medoids clustering (mixed data type) test for stationarity (Augmented Dickey-Fuller test), normality (Shapiro Wilk), conditinal heteroscedasticity (ARCH LM), Box-Cox transformation 7.2 SQL | Python | R | Tableau 7.2.1 Basic Statistics: Confidence intervals Tests of Hypotheses T-tests, correlation, regression, analysis of variance, chi-square tests 7.3 Probability: (usually test easy-medium prob questions) Probability spaces as models for phenomena with statistical regularity Discrete spaces (binomial, hypergeometric, Poisson) Continuos spaces (normal, exponential) and densities Random Variables | Expectation | Independence Conditional probability The Laws of large numbers Central Limit Theorem (CLT) MOST ASKED MATH TOPICS: PROB: probability theorems sampling &amp; experimental design random variables, PDF/CDF, expected values, etc. STATS book: Practical statistics for data scientists (Bruce) | engineering statistics handbook (NIST) | Statistical inference (Casella &amp; Berger) A/B Testing, hypothesis test differences of every type of distribution and how to test such type OLS and logistic 7.4 ML Theory (for DS-Modeling job) book: Introduction to Statistical Learning | Elements of statistical learning | Hands-on ML with Scikit Learn, Keras, and TensorFlow (by OREILLY) bias variance | sampling | under/overfitting | feature engineering | model evaluation how to select an apropriate model and features pros and cons of different models ETL pipeline/model, prod readiness 7.5 A/B Testing and Product Sense 7.6 Statistical Testing parametric vs. non-parametric each method’s assumptions and pros &amp; cons differences: parametric –&gt; independent, unbiased sample | non-parametric –&gt; ? case studies -- experiment design: mean, standard deviation, median, IQR Hypothesis testing h0 / h1 choose statistical test and significance level test statistics correlation (pearson / Spearman rank / Kendall rank) one-sample / two sample t-test | ANOVA | chi-square –&gt; goodness of fit, contigency test distribution histogram, skewness, kurtosis KS (Kolmogorov-Smirnov) test Shapiro-Wilk 68-95-99.7 7.7 Behavioral Questions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
